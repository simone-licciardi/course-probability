% 99_exercises.tex
%! TeX root = ../main.tex

\chapter*{Bassetti Unbound}
\addcontentsline{toc}{chapter}{Exercises}

\begin{my_ex}
	Let $\Omega$ be a set and let $A \subset \Omega$ a subset from it. Then, show that $\{ A, A^\mathsf{c}, \emptyset, \Omega \}$ is a $\sigma$-algebra.
\end{my_ex}
\begin{my_notes}
	This is the easiest nontrivial $\sigma$-algebra. It models a bet: the event may either happen or not (or nor could happen, that is the same as all the outcomes being realized). (??)
\end{my_notes}

\begin{my_ex}
	Let $\{ \mathcal{F}_\alpha \}_{\alpha \in I}$ be a colletion of $\sigma$-algebras. Is $\bigcap_{\alpha \in I} \mathcal{F}_\alpha$ a $\sigma$-algebra. What about $\bigcup_{\alpha \in I} \mathcal{F}_\alpha$?
\end{my_ex}
\begin{my_notes}
	This\footnote{The answer is that the first is, in fact, a $\sigma$-algebra, while the second not so, as it does not contain crossed unions and intersections.} justifies minimality arguments on the function $\sigma(\cdot)$. Read this \href{https://math.stackexchange.com/questions/54172/the-sigma-algebra-of-subsets-of-x-generated-by-a-set-mathcala-is-the-s/}{masterpiece}, this \href{https://groups.google.com/g/sci.math/c/DjVj6RiXOLs/m/PSMsTtfEnO8J}{essay} and this very general and technical \href{https://ncatlab.org/nlab/show/Moore+closure}{site} .
\end{my_notes}

\begin{my_ex}
	Prove the well-definiteness of $\sigma(\mathcal{E})$ as the minimal $\sigma$-algebra containing $\mathcal{E}$.
\end{my_ex}
\begin{my_notes}[Sketch of proof]
	Let $\Sigma(\mathcal{E})$ be the collection of all the $\sigma$-algebras containing the collection $\mathcal{E}$ of subsets of $\Omega$. (Prove that) $\Sigma(\mathcal{E})$ is not empty, and the family intersects to $\bigcap_{S\in\mathcal{E}}S = \sigma(\mathcal{E})$.
\end{my_notes}

\begin{my_ex}
	Let $E_1$,$E_2$ be events of $\Omega$. Think of a sample space and construct a measure of probability $\mathbb{P} : \mathcal{P}(\Omega) \to \mathbb{R}$ such that the two events are independent and $\mathbb{P}(E_1)=\mathbb{P}(E_2)=\frac{1}{2}$.
\end{my_ex}
\begin{my_notes}
	(??) % tip was to use uniform probability 
\end{my_notes}

\begin{my_ex}
	Let $\Omega=\mathbb{N}$. Show that $\mathrm{p}(\{n\})=\theta^n(1-\theta)$ for all $n\in\mathbb{N}$ is a discrete probability density. That is, show that it is coherent enough for a probability extending it to $\mathcal{P}(\mathbb{N})$ to exist.  
\end{my_ex}
\begin{my_notes}
	(??)
\end{my_notes}

\begin{my_ex}
	Let $E_1$,$E_2$ be independent events on $\Omega$ such that $\mathrm{p}(E_1)=\mathrm{p}(E_2)=\frac{1}{2}$. Determine the $sigma$-algebra, and find the probability $\mathbb{P}$ on this space consistent with the two values of $\mathrm{p}$.
\end{my_ex}
\begin{my_ex}
	% esercizio finale esercitazione 12 / 3
\end{my_ex}
\begin{my_remark}
	These are some examples of probability modelization Note that $\Omega$ is basically irrelevant. Here independence, a property \textit{of the probability}\footnote{Independence and conditioal probabilities are the charachteristic that really distinguish a probability from a measure.}, furnishes the necessary information for $\mathbb{P}$ to be defined uniquely. It allows us to work in a context of minimal information, by relating different partitions (that is, states).
\end{my_remark}

\begin{my_ex}[Jacod Protter, 7.1]
	\label{ex_jp_7_1}
	
\end{my_ex}
\begin{my_remark}
	The idea is that only finitely many disjoint events can have probability $\mathbb{P}(E) \leq \alpha$. That is all infinite sequences (convergent or divergent doesn't really matter, as we can restrict ourselves to the $\lim \sup$) need to tend to zero.
\end{my_remark}

\begin{my_ex}[Jacod Protter, 7.2]
	
\end{my_ex}
\begin{my_remark}
	\label{ex_jp_7_2}
	Same idea as in \ref{ex_jp_7_1}, but the fact that here we also apply results about cardinality. That is we group events by having a probability larger than $\frac{1}{n}$ and then use 7.1 to show that their cardinality need be discrete, as the whole collection is countable union of finite collections.
\end{my_remark}

\begin{my_ex}[Jacod Protter, 7.10]
	
\end{my_ex}
\begin{my_remark}
	This is an analytical result, but shows that the definition of discrete random variables is coherent with its charachterization in terms of cumulative density function. 
	
	To prove the result, one could create a bijection between $\mathbb{N}$ and the set of jump discontinuities $\mathbb{D}$, by using monotonicity and order on reals.
	A more instructive approach, though, is that of \ref{ex_jp_7_2}. We consider for each $n$ the set
	\[ 
		D_n = 
		\left\{
			x_0 \in [0,1] : 
			\text{ in } x_0 \text{ is located a jump discontinuity larger than } 
			\frac{1}{n}
		\right\}.
	\]
	By boundedness of $[0,1]$, $D_n$ need be finite. Then, $\bigcup_{n\in\mathbb{N}} D_n$ is discrete. 
	
	Analitically, you could also show that removable discontinuities need be discrete. See \href{run:assets/removable_disc.pdf}{here} for further considerations. This does not have direct applications in probability.
	% ISSUE WITH HREF^
\end{my_remark}

\begin{my_ex}
	Let
	\[
		F(x)=
		\begin{cases}
			0 & \text{if } x \leq 0 \\
			1-e^{-\lambda x} & \text{if } x > 0.
		\end{cases}	
	\]
	Show that this is a CDF. Does there exist, and if so, is it unique, the distribution $P$ that generates $F$? If it exists and is unique, find it.
	
	This distribution is denoted by $\mathcal{E}(\lambda)$ and is called the \textit{negative exponential} distribution.
	\end{my_ex}
	\begin{my_remark}
	For $0<a<b<+\infty$,
	\[
		P((a,b]) = e^{-a} - e^{-b}.	
	\]
	\end{my_remark}
	
	\begin{my_ex}
	Find an example of $X$ and $Y$ random variables from a measurable space such that $P_X = P_Y$ but $P(X=Y) \neq 1$.
	\end{my_ex}
	\begin{my_remark}
	Start by setting $(\Omega_1,\mathcal{F}_1,\mathbb{P}_1)=(\Omega_2,\mathcal{F}_2,\mathbb{P}_2)$. Then, observe that
	\[
		\mathbb{P} \{ X = Y \} = \mathbb{P} \{ \omega : X( \omega ) =Y( \omega ) \} = \mathbb{P} \{ \omega : X( \omega ) = t, Y( \omega ) = t, \text{ for } t \in \mathbb{R} \}
	\]
	and comparatively,
	\[
		P_X = P_Y \implies \mathbb{P} \{ X \in A \} = \mathbb{P} \{ Y \in A \},
	\]
	for every $A \in \mathcal{B}$.
	
	In other words, the first condition requires that the random variables have indistinguishable (probabilistically) preimages, while the second condition only requires that the probability of preimage sets be equal: the identity in the first case is element-wise, in the second, it's about the law.
	
	Idea: The symmetry of the law of $X$ and $Y$ produces structures indistinguishable by the law but with different relations.
	
	Solution: For $\Omega = \{a,b,c\}$ with $\mathbb{P}(\{a\})=\mathbb{P}(\{c\})=\frac{1}{4}$ and $\mathbb{P}(\{b\})=\frac{1}{2}$, and $X: (a,b,c)\to(1,0,-1)$, $Y: (a,b,c)\to(-1,0,1)$, the two random variables will be symmetric, fulfilling the required condition. Moreover, if $\Omega$ contains only $2$ elements, then $P\{X=Y\}=0$.
	\end{my_remark}
	
	\begin{my_ex}
	Let $U(0,1):(\Omega,\mathcal{F},\mathbb{P}) \to (\mathbb{R},\mathcal{B})$ be the random variable with a uniform distribution between $0$ and $1$.
	Explicitly,
	\[
		F_U(x)=
		\begin{cases}
			0 & \text{if } x < 0 \\
			x & \text{if } 0 \leq x < 1 \\
			1 & \text{if } x \geq 1.
		\end{cases}	
	\]
	Find the distribution of
	\[
		X : 
		\omega 
		\in (\Omega,\mathcal{F},\mathbb{P}) 
		\to  
		- \log ( U( \omega ) ) \mathbb{1} \{U( \omega ) > 0\} 
		\in (\mathbb{R},\mathcal{B}).	
	\]
	\end{my_ex}
	\begin{my_remark}
	The problem of finding the distribution of a random variable $Y$ can be approached in two ways:
	\begin{enumerate}
		\item If $Y$ is discrete, it suffices to find the the PMF, i.e. determine the support $S$ first and then the value $\mathbb{P} (X = s)$ for each $s \in S$. $P_X$ is the image law.
		\item If $Y$ is not discrete, it is not possible to describe $P_X$ by finding its value on atoms. It is necessary to find its value on a $\pi$-system. Often, this means finding the CDF $F_X$.
	\end{enumerate}
	In this case, we obviously adopt the second approach.
	
	Idea: See the function as the transformation of $U$ through a function $t$.
	
	Idea: Work with the monotonically increasing and invertible composite function to determine the interval for which you want to find the preimage with respect to $U$, and then solve using $F_U$.
	\end{my_remark}
	