% 99_exercises.tex
%! TeX root = ../main.tex

\chapter*{Bassetti Unbound}
\addcontentsline{toc}{chapter}{Exercises}

\begin{my_ex}
	Let $\Omega$ be a set and let $A \subset \Omega$ a subset from it. Then, show that $\{ A, A^\mathsf{c}, \emptyset, \Omega \}$ is a $\sigma$-algebra.
\end{my_ex}
\begin{my_notes}
	This is the easiest nontrivial $\sigma$-algebra. It models a bet: the event may either happen or not (or nor could happen, that is the same as all the outcomes being realized). (??)
\end{my_notes}

\begin{my_ex}
	Let $\{ \mathcal{F}_\alpha \}_{\alpha \in I}$ be a colletion of $\sigma$-algebras. Is $\bigcap_{\alpha \in I} \mathcal{F}_\alpha$ a $\sigma$-algebra. What about $\bigcup_{\alpha \in I} \mathcal{F}_\alpha$?
\end{my_ex}
\begin{my_notes}
	This\footnote{The answer is that the first is, in fact, a $\sigma$-algebra, while the second not so, as it does not contain crossed unions and intersections.} justifies minimality arguments on the function $\sigma(\cdot)$. Read this \href{https://math.stackexchange.com/questions/54172/the-sigma-algebra-of-subsets-of-x-generated-by-a-set-mathcala-is-the-s/}{masterpiece}, this \href{https://groups.google.com/g/sci.math/c/DjVj6RiXOLs/m/PSMsTtfEnO8J}{essay} and this very general and technical \href{https://ncatlab.org/nlab/show/Moore+closure}{site} .
\end{my_notes}

\begin{my_ex}
	Prove the well-definiteness of $\sigma(\mathcal{E})$ as the minimal $\sigma$-algebra containing $\mathcal{E}$.
\end{my_ex}
\begin{my_notes}[Sketch of proof]
	Let $\Sigma(\mathcal{E})$ be the collection of all the $\sigma$-algebras containing the collection $\mathcal{E}$ of subsets of $\Omega$. (Prove that) $\Sigma(\mathcal{E})$ is not empty, and the family intersects to $\bigcap_{S\in\mathcal{E}}S = \sigma(\mathcal{E})$.
\end{my_notes}

\begin{my_ex}
	Let $E_1$,$E_2$ be events of $\Omega$. Think of a sample space and construct a measure of probability $\mathbb{P} : \mathcal{P}(\Omega) \to \mathbb{R}$ such that the two events are independent and $\mathbb{P}(E_1)=\mathbb{P}(E_2)=\frac{1}{2}$.
\end{my_ex}
\begin{my_notes}
	(??) % tip was to use uniform probability 
\end{my_notes}

\begin{my_ex}
	Let $\Omega=\mathbb{N}$. Show that $\mathrm{p}(\{n\})=\theta^n(1-\theta)$ for all $n\in\mathbb{N}$ is a discrete probability density. That is, show that it is coherent enough for a probability extending it to $\mathcal{P}(\mathbb{N})$ to exist.  
\end{my_ex}
\begin{my_notes}
	(??)
\end{my_notes}

\begin{my_ex}
	Let $E_1$,$E_2$ be independent events on $\Omega$ such that $\mathrm{p}(E_1)=\mathrm{p}(E_2)=\frac{1}{2}$. Determine the $sigma$-algebra, and find the probability $\mathbb{P}$ on this space consistent with the two values of $\mathrm{p}$.
\end{my_ex}
\begin{my_ex}
	% esercizio finale esercitazione 12 / 3
\end{my_ex}
\begin{my_remark}
	These are some examples of probability modelization Note that $\Omega$ is basically irrelevant. Here independence, a property \textit{of the probability}\footnote{Independence and conditioal probabilities are the charachteristic that really distinguish a probability from a measure.}, furnishes the necessary information for $\mathbb{P}$ to be defined uniquely. It allows us to work in a context of minimal information, by relating different partitions (that is, states).
\end{my_remark}

\begin{my_ex}[Jacod Protter, 7.1]
	\label{ex_jp_7_1}
	
\end{my_ex}
\begin{my_remark}
	The idea is that only finitely many disjoint events can have probability $\mathbb{P}(E) \leq \alpha$. That is all infinite sequences (convergent or divergent doesn't really matter, as we can restrict ourselves to the $\lim \sup$) need to tend to zero.
\end{my_remark}

\begin{my_ex}[Jacod Protter, 7.2]
	
\end{my_ex}
\begin{my_remark}
	\label{ex_jp_7_2}
	Same idea as in \ref{ex_jp_7_1}, but the fact that here we also apply results about cardinality. That is we group events by having a probability larger than $\frac{1}{n}$ and then use 7.1 to show that their cardinality need be discrete, as the whole collection is countable union of finite collections.
\end{my_remark}

\begin{my_ex}[Jacod Protter, 7.10]
	
\end{my_ex}
\begin{my_remark}
	This is an analytical result, but shows that the definition of discrete random variables is coherent with its charachterization in terms of cumulative density function. 
	
	To prove the result, one could create a bijection between $\mathbb{N}$ and the set of jump discontinuities $\mathbb{D}$, by using monotonicity and order on reals.
	A more instructive approach, though, is that of \ref{ex_jp_7_2}. We consider for each $n$ the set
	\[ 
		D_n = 
		\left\{
			x_0 \in [0,1] : 
			\text{ in } x_0 \text{ is located a jump discontinuity larger than } 
			\frac{1}{n}
		\right\}.
	\]
	By boundedness of $[0,1]$, $D_n$ need be finite. Then, $\bigcup_{n\in\mathbb{N}} D_n$ is discrete. 
	
	Analitically, you could also show that removable discontinuities need be discrete. See \href{run:assets/removable_disc.pdf}{here} for further considerations. This does not have direct applications in probability.
	% ISSUE WITH HREF^
\end{my_remark}

\begin{my_ex}
	Let
	\[
		F(x)=
		\begin{cases}
			0 & \text{if } x \leq 0 \\
			1-e^{-\lambda x} & \text{if } x > 0.
		\end{cases}	
	\]
	Show that this is a CDF. Does there exist, and if so, is it unique, the distribution $P$ that generates $F$? If it exists and is unique, find it.
	
	This distribution is denoted by $\mathcal{E}(\lambda)$ and is called the \textit{negative exponential} distribution.
	\end{my_ex}
	\begin{my_remark}
	For $0<a<b<+\infty$,
	\[
		P((a,b]) = e^{-a} - e^{-b}.	
	\]
	\end{my_remark}
	
	\begin{my_ex}
	Find an example of $X$ and $Y$ random variables from a measurable space such that $P_X = P_Y$ but $P(X=Y) \neq 1$.
	\end{my_ex}
	\begin{my_remark}
	Start by setting $(\Omega_1,\mathcal{F}_1,\mathbb{P}_1)=(\Omega_2,\mathcal{F}_2,\mathbb{P}_2)$. Then, observe that
	\[
		\mathbb{P} \{ X = Y \} = \mathbb{P} \{ \omega : X( \omega ) =Y( \omega ) \} = \mathbb{P} \{ \omega : X( \omega ) = t, Y( \omega ) = t, \text{ for } t \in \mathbb{R} \}
	\]
	and comparatively,
	\[
		P_X = P_Y \implies \mathbb{P} \{ X \in A \} = \mathbb{P} \{ Y \in A \},
	\]
	for every $A \in \mathcal{B}$.
	
	In other words, the first condition requires that the random variables have indistinguishable (probabilistically) preimages, while the second condition only requires that the probability of preimage sets be equal: the identity in the first case is element-wise, in the second, it's about the law.
	
	Idea: The symmetry of the law of $X$ and $Y$ produces structures indistinguishable by the law but with different relations.
	
	Solution: For $\Omega = \{a,b,c\}$ with $\mathbb{P}(\{a\})=\mathbb{P}(\{c\})=\frac{1}{4}$ and $\mathbb{P}(\{b\})=\frac{1}{2}$, and $X: (a,b,c)\to(1,0,-1)$, $Y: (a,b,c)\to(-1,0,1)$, the two random variables will be symmetric, fulfilling the required condition. Moreover, if $\Omega$ contains only $2$ elements, then $P\{X=Y\}=0$.
\end{my_remark}
	
\begin{my_ex}
	Let $U(0,1):(\Omega,\mathcal{F},\mathbb{P}) \to (\mathbb{R},\mathcal{B})$ be the random variable with a uniform distribution between $0$ and $1$.
	Explicitly,
	\[
		F_U(x)=
		\begin{cases}
			0 & \text{if } x < 0 \\
			x & \text{if } 0 \leq x < 1 \\
			1 & \text{if } x \geq 1.
		\end{cases}	
	\]
	Find the distribution of
	\[
		X : 
		\omega 
		\in (\Omega,\mathcal{F},\mathbb{P}) 
		\to  
		- \log ( U( \omega ) ) \mathbb{1} \{U( \omega ) > 0\} 
		\in (\mathbb{R},\mathcal{B}).	
	\]
\end{my_ex}
\begin{my_remark}
	The problem of finding the distribution of a random variable $Y$ can be approached in two ways:
	\begin{enumerate}
		\item If $Y$ is discrete, it suffices to find the the PMF, i.e. determine the support $S$ first and then the value $\mathbb{P} (X = s)$ for each $s \in S$. $P_X$ is the image law.
		\item If $Y$ is not discrete, it is not possible to describe $P_X$ by finding its value on atoms. It is necessary to find its value on a $\pi$-system. Often, this means finding the CDF $F_X$.
	\end{enumerate}
	In this case, we obviously adopt the second approach.
	
	Idea: See the function as the transformation of $U$ through a function $t$.
	
	Idea: Work with the monotonically increasing and invertible composite function to determine the interval for which you want to find the preimage with respect to $U$, and then solve using $F_U$.
\end{my_remark}

\begin{my_ex}[JP, 9.5]
	%
\end{my_ex}
\begin{my_remark}
	This shows that the role of e.v. on a class of functions is similar to that of the probability on a $\sigma$-algebra, a case which can be found for $X=1$.
\end{my_remark}

\begin{my_ex}
	Consider the function $Q:\mathcal{F}\to\mathbb{R}$ such that
	\[
		Q(A) = \int_A f \:\mathrm{d} m,
	\]
	where $f$ is a PMF (that is, $f$ is measurable, $f \leq 0$, $\int_\mathbb{R} f \:\mathrm{d}m$).	Show that it is a probability.
\end{my_ex}
\begin{my_remark}
	The result is a particular case of the precedent exercise. Since we consider a PMF $f$, it is equivalent to restrict the $X$ of exercise [JP, 9.5] to absolutely continuous r.v.'s.
\end{my_remark}

\begin{my_ex}
	Let $\left( \Omega, \mathcal{F}, \mathbb{P} \right)= \left( \mathbb{R}, \mathcal{B}, \mathcal{E}(\lambda) \right)$ for $\lambda>0$, where we recall that $f_\mathcal{E}(x)=\lambda e^{-\lambda x}$.
	\begin{enumerate}
		\item Consider $X: \omega \in \Omega \to \lfloor \omega \rfloor \in \mathbb{R}$. Show that $X \sim \mathcal{G}(1-e^{-\lambda})$, that is that the image law is a geometric distribution.
		\item Compute $\mathbb{E}[X]$ directly, that is without making use of the expectation rule.
		\item Compute $\mathbb{E}[X]$ using the expectation rule, and compare the proceedings with the above.
	\end{enumerate}
\end{my_ex}
\begin{my_remark}
	First, observe that since $X(\Omega)=\mathbb{Z}$, $X$ is discrete since $P_X(\mathbb{Z})=1$. Moreover, the support is just $\mathbb{N}$, since the exponential distribution is null for negatives and so we can consider instead $\tilde{X}$, a.s. agreeing, that is null for $x<0$. Then, it suffices to find the PMF:
	\begin{equation*}
		p_x(n)=
		\mathbb{P}\left\{X=n\right\}=
		\mathbb{P}\left\{X=n\right\}=
		\mathbb{P}\left( \left( n,n+1 \right] \right)=
		\int_{\left( n,n+1 \right]} f \:\mathrm{d}m=
		\int_{n}^{n+1} \lambda e^{-\lambda t} \:\mathrm{d}t=
		\left( 1 - e^{-\lambda} \right) e^{-\lambda n}.
	\end{equation*}
	This is, in fact, a geometric distribution. For the second point, we first verify that the expected value exists: by the above discussion, since $\tilde{X}=X$ a.s. and since $\tilde{X} \leq 0$, it follows that $X$ admitts e.v. We can find it with the following equialities: 
	\begin{multline*}
		\mathbb{E}[X] = \int_\Omega \lfloor \omega \rfloor \:\mathrm{d}\mathbb{P}(\omega) = \int_\Omega \sum^{\infty} k\mathbb{1}_{\left( n,n+1 \right]} \:\mathrm{d}\mathbb{P}(\omega) = \sum^{\infty} k \int_\Omega \mathbb{1}_{\left( n,n+1 \right]} \:\mathrm{d}\mathbb{P}(\omega) =\\= \sum^{\infty} k \mathbb{P} \left(\left( n,n+1 \right]\right) = \frac{1}{1 - e^{- \lambda}}.
	\end{multline*}
	We have used the corollary for series to the MCT in order to commute integration and summation (note that the r.v. is positive and so this is allowed). The last passage is motivated by the known result
	\[
		\sum^{\infty} k \, t^{k-1} = \frac{1}{(1-t)^2}.	
	\]
	With the expectation rule, this gets notably shortened.
	\[
		\mathbb{E}[X] = \int_\mathbb{R} \mathrm{id} \:\mathrm{d}P_X = \int_\mathbb{R} \lfloor x \rfloor \:\mathrm{d}P_X(x) = \sum_{k\in\mathbb{N}} k p_X(k) = \frac{1}{1 - e^{- \lambda}}.
	\]
	Thus, the expectation rule, known as e.r., can make computations much easier. Not only that, but if we were just given the image law of X and not the probabiltiy on $\left(\Omega,\mathcal{F}\right)$, we would have still been able to compute the expected value.
\end{my_remark}

\begin{my_ex}
	Find the e.v. and var. of the discrete uniform distribution.
\end{my_ex}
\begin{my_remark}
	Let $X\sim\mathrm{d}\mathcal{U}(\{1,\dots,n\})$. Then, the e.v. exists because the $X$ is positive, and $\mathbb{E}[X] = \frac{n+1}{2}$. Moreover, $\mathrm{Var}(X)=\mathbb{E}[X^2]-\mathbb{E}^2[X]=\frac{n^2-1}{12}$. Where the computation of the second moment comes from the closed form for the sum
	\[
	\sum^n_{k=0}  k^2 = \frac{n(2n+1)(n+1)}{6}.
	\]
\end{my_remark}

\begin{my_ex}
	By computing the e.v. of the distribution $p(k)=\frac{6}{\pi^2}\frac{1}{k^2}$ of $\mathbb{N}$, show that the expected value need not be finite. Otherwise said, show that there are r.v. that admitt the e.v., but not variance.
\end{my_ex}
\begin{my_remark}
	As the distribution is positive, one has that the e.v. exists. Moreover, $X$ is discrete, and so
	\[
		\mathbb{E}[X] = \sum ^ \infty k p(k) = \frac{6}{\pi^2} \sum^\infty \frac{1}{k} = \infty.	
	\]
	Obviusly, $X \notin L^1$ and so $\mathrm{Var}$ is not defined.
\end{my_remark}