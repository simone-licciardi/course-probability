% 01_complements.tex
%! TeX root = ../main.tex

\chapter*{Complements to Chapter 1}
\addcontentsline{toc}{chapter}{Complements}
% \begin{document}
\section{Probability construction}

Operatively, there are two ways of furnishing a (measure of) probability on a measurable space \measurablespace: it may be claimed or inferred. 
In the first case, its law is given \textit{a priori}, while in the second it is deducted \textit{a posteriori}, from the knowledge of the probability values on some elementary events, that is.

The first is common in applications such as Bayesian Statistics, where you make an hypothesis on the distribution and then test it, while the second is also characteristic of measure theory. 
The key difference lies in the fact that we either have complete or minimal information about the probability.

In both cases some check are in order, as we need to ensure that the probability is \textit{coherent}. 
Note that in the second case, we also need to ensure that the one generated is unique, the minimal information is \textit{sufficient}, that is. 

\subsection{Discrete setting}

Some results of this kind were produced already: we have dealt with the case of discrete partitions of the sample space $\Omega$.
We quote such theorems.

\begin{my_theorem}[Existence and Uniqueness]
	\label{disc_partition}
	Let $\dispart$ be a discrete partition of $\Omega$ and suppose that the function $\p:E_k \in \mathcal{E} \to p_k \in \mathbb{R}$ satisfies
	\begin{subnumcases}{}
		\sum_{k\in I} p_k=1 \label{normalization}
		\\
		p_k\geq0 \quad\textit{for all } k \in I.\label{positiveness}
	\end{subnumcases}
	Then, there exist a unique probability $\mathbb{P}$ on $\sigma(\mathcal{E})$ such that $\mathbb{P}$ and $\p$ agree on $\mathcal{E}$. A function $\p$ with the properties (\ref{normalization}) and (\ref{positiveness}) of is called a discrete probability density.
\end{my_theorem}
(??) % add a remark about to explain why a partition is useful in first place.

Morally, the probability density on a discrete partition can be extended with consistency to a unique probability on the generated \sigmaalg. 
Moreover, in this setting we can describe explicitly each event $E\in\sigma(\mathcal{E})$ and its probability $\mathbb{P}(E)$.

\begin{my_lemma}
	\label{explicit_char}
	Let $\dispart$ be a discrete partition of $\Omega$. Then
	\[
		\sigma( \mathcal{E} ) = \left\{ \, \bigcup_{k \in J} E_k \, \text{ for }J \subset I \right\}.
	\]
\end{my_lemma}
\begin{my_lemma}
	Let $\p : E_k \in \mathcal{E} \to p_k \in \mathbb{R}$ be a discrete probability density. Then, if $\mathbb{P}$ is the probability defined in Theorem \ref{disc_partition},
	\[
		\mathbb{P} \left( \, \bigcup_{k \in J} E_k \,\right) = \sum_{k \in J} \p_k \quad \text{ for all }J \subset I.
	\]
\end{my_lemma}
 
\begin{my_remark}
	A special case worth mentioning is that of the atomic partition on a discrete $\Omega$. 
	Here, the $\sigma$-algebra generated is $\mathcal{P}(\Omega)$ and the discrete probability density is commonly referred to as $p(\{ \omega \})=p_\omega$. 
	By the precedent results, $p$ defines a unique probability $\mathbb{P}$ consistent over $\mathcal{E}$ such that
	\begin{equation}
		\mathbb{P} \left( E \right) = \sum_{\omega \in E} p_\omega \quad \text{ for all }E \subset \Omega.		
	\end{equation}
\end{my_remark}

But $\mathbb{P}$ need not be given over just a single partition and, actually, since a probability is a very special kind of measure, it is not even necessary that the information about it is values! We will explain the matter thorugh examples, but first we state an handy result.
\begin{my_corollary}
	\label{cond_partitions}
	Let $\mathcal{E}=\{ E_k \}_{k\in I}$ be a partition of $\Omega$ and suppose that the function $\p:E_k \in \mathcal{E} \to p_k \in \mathbb{R}$ satisfies $\sum_{k\in I} p_k=1$ and $p_k\geq0$ for all $k \in I$. Moreover, let $\mathcal{F}=\{ F_h \}_{h\in J}$ and suppose that the function $\mathrm{q}_k:F_h \in \mathcal{F} \to q_{h\,|k} \in \mathbb{R}$ satisfies
	\[
		\sum_{h \in J} q_{h\,|k}=1 \quad \text{once $k$ is fixed, and } p_i > 0
		\\
		q_{h\,|k}\geq0 \quad \text{for all } k \in I \text{ and } h \in J.
	\]
	Then, there exist a unique probability $\mathbb{P}$ on $\sigma(\mathcal{E}\cup\mathcal{F})$ such that $\mathbb{P}$ and $\p$ agree on $\mathcal{E}$ and
	\[
		\mathbb{P}\left( F_h | E_k \right) = q_{h|k} \quad \text{for all } k \in I \text{ and } h \in J.
	\]
\end{my_corollary}
The result benefits some explaining. 
\begin{my_remark}
	\label{real_part}
	The difference with Theorem \ref{disc_partition} is that the $\sigma$-algebra is \textit{quite bigger}. To understands \textit{how much}, it is sufficient to observe that it can also be charachterized as the $\sigma$-algebra generated by the partition $\mathcal{Q}=\{ E_k \cap F_h\}_{k\in I,h\in J}$ of $\Omega$. 
\end{my_remark}	
\begin{my_remark}
	\label{usefull_condpart}
	The result states under which conditions, a tree such as the following contains enough information for the probability to be defined and unique.
	(??)% image
	Recall Remark 
	(??) %\ref{usefull_part}. 
	In probability, it is frequent to study the relationship between different states, and there lies the usefulness of this result as well. It guarantees that if we were given not only the probability of a state, but also the probability of another state \textit{in relation with} the first, then we can furnish consistently and uniquely the probability over combinations of all sorts of these states.
\end{my_remark}
\begin{my_example}
	
\end{my_example}
\begin{my_remark}
	Recall Remark \ref{usefull_condpart}. A special case is that where the information correlating states is independence. In that case, we can do one of two things depending on the request of the problem. First, we can use the definition: that is put $\mathbb{P}(F_h \cap E_k)=\mathbb{P}(F_h)\mathbb{P}(E_k)$ and find the probabilities of the explicit partition in Remark \ref{real_part}. Then, apply Theorem \ref{disc_partition} in order to find $\mathbb{P}$ on every event. Second, we make use of the fact that for $\mathbb{P}(E_k)>0$ independence is equivalent to $\mathbb{P}(F_h|E_k)=\mathbb{P}(F_h)$. Once we establish all the conditional probabilities, by Theorem \ref{cond_partitions} we are guaranteed that the probability exists. Finding the values of $\mathbb{P}$ is then a matter of manipulating conditional probabilities.
	%$\mathbb{P}(F_h | E_k)=\mathbb{P}(F_h)$.
\end{my_remark}
\begin{my_example}
	
\end{my_example}
Moreover, this theorem allows us to easy the question of probability modelisation for repeated experiments. (??)
% esercitazione di prov del 12 /3, discorso di greg su quando dire cge le prob delle entries sono aleatorie, allora anche il vettore esito lo è.

\subsection{Carathéodory Theorem}

For more general settings the short message is that Theorem \ref{disc_partition} holds, provided some conditions, while no explicit characterization like that of Lemma \ref{explicit_char} is possible.

We present a powerful theorem of measure theory, that does just that. It will allow us to extend a \textit{pre-probability}, a function with some coherence that is defined on a collection smaller than a \sigmaalg , to a probability, in a unique fashion.

\goodbreak\begin{my_definition}
	\label{proprob}
	Let $\alg$ be an algebra defined on $\Omega$. Then, $\preprob:\alg\to\mathbb{R}$ is a \textit{pre-probability} if
	\begin{enumerate}
		\item $\preprob( \Omega ) = 1$ \hfill (Normalization)
		\item $\preprob \left( \, \bigcup^n_{i=1} A_i \, \right) = \sum^n_i \preprob(A_i)$ for $A_i\in\alg$ \hfill (Additivity)
		\item if $\bigcup_{ i \in \mathbb{N} } A_i \in \alg$, then $\preprob \left( \,\bigcup_{ i \in \mathbb{N} } A_i \,\right) = \sum_{ i \in \mathbb{N}} \preprob(A_i)$.
	\end{enumerate}
\end{my_definition}
\begin{my_remark}
	The latter is the condition that ensure coherence with respect to the probability, and can be read as a \textit{need-based \sigmaadd}.
\end{my_remark}

\goodbreak\begin{my_theorem}[Carathéodory's Theorem]
	\label{carathéorodory}
	Let $\alg$ be an algebra defined on $\Omega$, and suppose that $\preprob : \alg \to \mathbb{R}$ is a pre-probability.	Then, there exists a unique probability $\mathbb{P}: \sigma ( \alg ) \to \mathbb{R}$ such that $\preprob$ and $\mathbb{P}$ agree on $\alg$.
\end{my_theorem}
\begin{my_remark}
	This version of the Carathéodory's Theorem is of theoretical interest and provides two results.
	First, it shows that \textbf{existence} of a consistent extension is guaranteed just by requiring the coherence of $\preprob$ with the conditions that the define a probability: these are encoded in Definition \ref{proprob}.
	Moreover, the theorem quantifies the idea that if $\preprob$, the information provided about $\mathbb{P}$ that is, is defined on a large enough collection, then its extension is \textbf{unique}. 
	In particular, we require $\mathbb{P}$ to be given on an algebra, a much smaller collection than a \sigmaalg.
\end{my_remark}

We can refine the result for practical purposes by exploiting the equivalence between $\sigma$-additivity and continuity\footnote{This result from measure theory is taken to be known and the details are out of the scope of these notes.}.

\goodbreak\begin{my_lemma}[Carathéodory's Theorem, continuity characterization]
	Let $\alg$ be an algebra defined on $\Omega$, and suppose that $\preprob:\alg\to\mathbb{R}$ satisfies
	\begin{enumerate}
		\item $\preprob(\Omega)=1$,
		\item $\preprob \left( \, \bigcup^n_{i=1} A_i \, \right) = \sum^n_i \preprob(A_i)$,
		\item if $A_i \, \big \downarrow \, \emptyset$, then $\preprob(A_i) \big \downarrow0$,
	\end{enumerate}
	where $A_n\in\alg$.
	Then, there exists a unique probability $\mathbb{P}: \sigma ( \alg ) \to \mathbb{R}$ such that $\preprob$ and $\mathbb{P}$ agree on $\alg$.
\end{my_lemma}
\begin{my_remark}
	The usefulness of this is that monotone continuity, that is showing a limit is $0$, is generally much easier than working on countable unions to arbitrary sets. 
	It is of theoretical interest that this only holds if we use an algebra: as we will see, if it wasn't for this characterization weaker conditions on the collection (namely, that it is a $\pi$-system instead of an algebra) could be used.
\end{my_remark}

\subsection{Practical construction, set structure}

An algebra is still a sizeable class, making it hard to define $\mathbb{P}$. This is what Dynkin’s $\lambda-\pi$ Theorem was designed to simplify.

\begin{my_definition}[$\pi$-system]
	Given a set $\Omega$, a collection of subsets $\mathcal{C}$ is a $\pi$-system if stable it is under finite intersection. Explicitly, if $A_1, \dots, A_n \in \mathcal{C}$ implies that $\bigcap^n_{i=1} A_i \in \mathcal{C}$.
\end{my_definition}
\begin{my_remark}
	Operatively, it suffices to show that $A \cap B \in \mathcal{C}$ whenever $A,B \in \mathcal{C}$, for $\mathcal{C}$ to be a $\pi$-system by inductive argument.
\end{my_remark}
\begin{my_definition}[$\lambda$-system]
	Given a set $\Omega$, a collection of subsets $\mathcal{C}$ containing $\Omega$ is a $\lambda$-system if it is stable under (pairwise) disjoint countable union and proper difference. Explicitly, if $A_1, A_2, \dots \in \mathcal{C}$ and $A_i \cap A_j = \emptyset$ for all $i$,$j$ implies that $\bigcup_{i=1} A_i \in \mathcal{C}$ and if $A,B \in \mathcal{C}$ implies that $A \setminus B \in \mathcal{C}$.
\end{my_definition}
\begin{my_remark}
	\label{proper_diff_stab}
	Proper difference stability is more intelligible under the light of its equivalence with complementation. Recall that set difference can be charachterized as follows:
	\[
		A \setminus B = A \cap B^\mathsf{C},
	\]
	where complementation is taken with respect to a space containing both A and B. In particular, if $B \subset A$ the right hand side boils down to the complementation of $B$ with respect to $A$. Thus, that a collection is closed under proper difference is the same as complementation stability with respect to all $A \in \mathcal{C}$. Formally, for all $A$ in $\mathcal{C}$ it holds that $\mathcal{G}_A=\left \{B \in \mathcal{C} \text{ such that } B \subset A \right \}$ is stable under complementation. 
	
	Importantly, proper difference stability implies complementation stability with respect to $\Omega$, since $\Omega \in \mathcal{C}$,.
\end{my_remark}
\begin{my_remark}
	Remark \ref{proper_diff_stab} highlight the difference between a $\lambda$-system and a $\sigma$-algebra: in the former the countale unions need to be \textit{disjoint}, while in the latter not so. An example of collection that is closed under disjoint union, but not union, is
	\[
		\mathcal{C}=\left \{ \emptyset, \{a,b\},\{c,d\},\{a,c\},\{b,d\}, \{a,b,c,d\} \right \}
	\]
\end{my_remark}
Let us investigate the relationship between a $\sigma$-algebra and these simpler systems. 
\begin{my_lemma}
	A collection is a $\sigma$-algebra if and onl if it is both a $\pi$-system and a $\lambda$-system.	
\end{my_lemma}
This is a much stronger relation than it looks, as the following theorem shows.
\begin{my_theorem}[$\lambda-\pi$ Theorem]
	\label{lambda-pi}
	If $\mathcal{C}$ is a $\pi$-system and $\mathcal{D}$ is a $\lambda$-system containing it, then $\sigma(\mathcal{C}) \in \mathcal{D}$.
\end{my_theorem}
\begin{my_remark}
	An equivalent statement would be that $\sigma(\mathcal{C}) = \lambda(\mathcal{C})$, where the latter is the smallest $\lambda$-system generated by $\mathcal{C}_\pi$. That this exists unique follows from the intersection of $\lambda$-systems being a $\lambda$-system.
\end{my_remark}
A similar result to \ref{lambda-pi} is the following, where we weaken the requirements on $\mathcal{D}$ so that it is only required to be a monotone class, at the expense of stronger stability for $\mathcal{C}$.
\begin{my_theorem}[Monotone Class Theorem, set version]
	\label{mct}
	If $\mathcal{A}$ is an algebra and $\mathcal{M}$ is a monotone class containing it, then $\sigma(\mathcal{A}) \in \mathcal{M}$.	
\end{my_theorem}
\begin{my_remark}
	Here, an equivalent statement would be that $ \sigma \left( \mathcal{A} \right) = \mathrm{M} \left( \mathcal{A} \right) $, where the latter is the smallest monotone system generated by $\mathcal{A}$ and existence and uniquess are as above.
\end{my_remark}
\begin{my_remark}
	One of the main applications of the Monotone Class Theorem is that of showing that certain property is satisfied by all sets in an $\sigma$-algebra, generally starting by the fact that the field generating the $\sigma$-algebra satisfies such property and that the sets that satisfies it constitutes a monotone class.
\end{my_remark}
% example here would fit well.
The following is an immediate application of Theorem \ref{carathéorodory} or \ref{mct} equivalently.
\begin{my_lemma}
	Let $\mathbb{P}_1$ and $\mathbb{P}_2$ be probabilities on $\sigma(\mathcal{C})$ and $\mathcal{C}$ be a $\pi$-system. If $\mathbb{P}_1$ and $\mathbb{P}_2$ agree on $\mathcal{C}$, then they agree on $\sigma(\mathcal{C})$.
\end{my_lemma}
\begin{my_remark}
	The lemma is of practical interest. Evidently, it allows easier comparison between probabilities, as it suffices to check equality on a much smaller collection than the domain. Importantly, the lemma also furnishes sharp\footnote{Mathematical gibberish for ``minimal''} conditions for uniqueness in Theorem \ref{carathéorodory}: it allows us to define the pre-probability on just a $\pi$-system containing $\Omega$, and deduce from that its unique extension.
\end{my_remark}

\subsection{Practical construction of probability}

This continues the precedent section, where we have shown how to divide the construction of the sigma algebra into its components. Now we  set as objective that of showing the passages of the most common practice in probability construction: that of defining the pre-probability on a $\pi$-system, extending it to a semi-algebra, and then to an algebra in order to apply Carathéodory. That of going from the $\pi$-system to the algebra operatively. 

% \href{http://theanalysisofdata.com/probability/E_3.html}{here}.




% see pdf in the wd.



\section*{Borel's $\sigma$-algebra}

\section*{$\sigma$-algebra on Bernoulli Space}

% \end{document}


%% mwe stuff:

% \documentclass[a4paper, 11pt]{article}

% \usepackage[english]{babel}
% \usepackage[hmargin=2cm,vmargin=2cm]{geometry}
% \usepackage{amsmath,amssymb,amsthm}
% \usepackage{cases}
% \usepackage{xcolor}
% \usepackage[colorlinks=true,linkcolor=black,urlcolor=blue]{hyperref}

% \newtheorem{my_theorem}{Theorem}[section]
% \newtheorem{my_lemma}[my_theorem]{Lemma}
% \newtheorem{my_corollary}[my_theorem]{Corollario}
% \newtheorem{my_property}[my_theorem]{Proprietà}
% 	\theoremstyle{definition}
% \newtheorem{my_definition}[my_theorem]{Definizione}
% 	\theoremstyle{remark}
% \newtheorem{my_remark}[my_theorem]{Remark}

% \newcommand{\salg}{\sigma\mathcal{A}}
% \newcommand{\varsalg}{\sigma\mathcal{F}}
% \newcommand{\alg}{\mathcal{A}}
% \newcommand{\varaalg}{\mathcal{F}}
% \newcommand{\dispart}{\mathcal{E}=\{ E_k \}_{k\in I}}
% \newcommand{\p}{\mathrm{p}}
% \newcommand{\preprob}{\tilde{\mathbb{P}}}

% \newcommand{\sigmaalg}{$\sigma$-algebra}
% \newcommand{\sigmaadd}{$\sigma$-additivity}
% \newcommand{\measurablespace}{$(\Omega,\mathcal{F})$}

% \setcounter	{footnote}	{1}
% \author{S. Licciardi\footnote{simone.licciardi@mail.polimi.it}, PoliMI undergraduate}
% \date{\large Anno accademico 2023-2024}
